{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91af6cb5-6ac3-461f-a289-66449f9e00bc",
      "metadata": {
        "id": "91af6cb5-6ac3-461f-a289-66449f9e00bc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import io\n",
        "import h5py\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau,LearningRateScheduler\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.keras.utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd82e6c0-a3f1-42a0-bc26-beb9d646f0cb",
      "metadata": {
        "id": "cd82e6c0-a3f1-42a0-bc26-beb9d646f0cb"
      },
      "outputs": [],
      "source": [
        "def check_gpu():\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            for gpu in gpus:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
        "            print(f\"Physical GPUs: {len(gpus)}, Logical GPUs: {len(logical_gpus)}\")\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "    return len(gpus) > 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bYXO8nDvbXcf",
      "metadata": {
        "id": "bYXO8nDvbXcf"
      },
      "outputs": [],
      "source": [
        "# electron_path = 'SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "# photon_path = 'SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "electron_path = '/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5'\n",
        "photon_path = '/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7mK5ETe22QHh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mK5ETe22QHh",
        "outputId": "c87bd3a0-9794-4960-fc0d-c972e6dbc4c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Electron file structure:\n",
            "- Dataset: X, Shape: (249000, 32, 32, 2), Dtype: float32\n",
            "- Dataset: y, Shape: (249000,), Dtype: float32\n"
          ]
        }
      ],
      "source": [
        "with h5py.File(electron_path, 'r') as f:\n",
        "    print(\"Electron file structure:\")\n",
        "    for key in f.keys():\n",
        "        item = f[key]\n",
        "        if isinstance(item, h5py.Dataset):  # If item is a dataset\n",
        "            print(f\"- Dataset: {key}, Shape: {item.shape}, Dtype: {item.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "uQ6-vwW4bXfM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ6-vwW4bXfM",
        "outputId": "3bbcd092-7859-4809-d7b3-6fa1f7cf942d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Photon file structure:\n",
            "- Dataset: X, Shape: (249000, 32, 32, 2), Dtype: float32\n",
            "- Dataset: y, Shape: (249000,), Dtype: float32\n"
          ]
        }
      ],
      "source": [
        "with h5py.File(photon_path, 'r') as f:\n",
        "    print(\"Photon file structure:\")\n",
        "    for key in f.keys():\n",
        "        item = f[key]\n",
        "        if isinstance(item, h5py.Dataset):  # If item is a dataset\n",
        "            print(f\"- Dataset: {key}, Shape: {item.shape}, Dtype: {item.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HE17bM6M7AQ2",
      "metadata": {
        "id": "HE17bM6M7AQ2"
      },
      "outputs": [],
      "source": [
        "# electron_path = h5py.File('/content/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5')\n",
        "# photon_path = h5py.File('/content/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "wHYF4NOZ9nOo",
      "metadata": {
        "id": "wHYF4NOZ9nOo"
      },
      "outputs": [],
      "source": [
        "def print_hdf5_structure(f, indent=0):\n",
        "    \"\"\"Helper function to print HDF5 file structure\"\"\"\n",
        "    for key in f.keys():\n",
        "        item = f[key]\n",
        "        if isinstance(item, h5py.Dataset):  # If item is a dataset\n",
        "            print(\" \" * indent + f\"- Dataset: {key}, Shape: {item.shape}, Dtype: {item.dtype}\")\n",
        "        elif isinstance(item, h5py.Group):  # If item is a group\n",
        "            print(\" \" * indent + f\"- Group: {key}\")\n",
        "            print_hdf5_structure(item, indent + 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "Dcf-M5aB-U0V",
      "metadata": {
        "id": "Dcf-M5aB-U0V"
      },
      "outputs": [],
      "source": [
        "\n",
        "def normalize_data(data):\n",
        "    \"\"\"Min-max normalization for each sample\"\"\"\n",
        "    result = np.zeros_like(data, dtype=np.float32)\n",
        "    for i in range(len(data)):\n",
        "        min_val = np.min(data[i])\n",
        "        max_val = np.max(data[i])\n",
        "        if max_val > min_val:\n",
        "            result[i] = (data[i] - min_val) / (max_val - min_val)\n",
        "        # If all values are the same, leave as zeros\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GOGnfuiIbXkU",
      "metadata": {
        "id": "GOGnfuiIbXkU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and preprocess the data\n",
        "def data_structure(electron_file, photon_file):\n",
        "    \"\"\"\n",
        "    Load electron and photon data from HDF5 files.\n",
        "\n",
        "    Args:\n",
        "        electron_file (str): Path to electron data file\n",
        "        photon_file (str): Path to photon data file\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, y_train, X_test, y_test)\n",
        "    \"\"\"\n",
        "    # Load electron data (class 0)\n",
        "    with h5py.File(electron_file, 'r') as f:\n",
        "        print(\"Electron file structure:\")\n",
        "        print_hdf5_structure(f)\n",
        "        try:\n",
        "            # First attempt with expected names\n",
        "            electron_energy = np.array(f['energy'])\n",
        "            electron_time = np.array(f['time'])\n",
        "        except KeyError:\n",
        "            # If those keys don't exist, look for other possible structures\n",
        "            if 'X' in f:\n",
        "                # If data is stored as a single dataset with multiple channels\n",
        "                data = np.array(f['X'])\n",
        "                print(f\"Found data with shape: {data.shape}\")\n",
        "\n",
        "                # Assuming data shape is (n_samples, 2, 32, 32) or (n_samples, 32, 32, 2)\n",
        "                if len(data.shape) == 4:\n",
        "                    if data.shape[1] == 2:  # (n_samples, 2, height, width)\n",
        "                        electron_energy = data[:, 0]\n",
        "                        electron_time = data[:, 1]\n",
        "                    elif data.shape[3] == 2:  # (n_samples, height, width, 2)\n",
        "                        electron_energy = data[..., 0]\n",
        "                        electron_time = data[..., 1]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected data dimensions: {len(data.shape)}\")\n",
        "            else:\n",
        "                # List all available keys and try to infer which ones contain the data\n",
        "                print(\"Available keys in electron file:\", list(f.keys()))\n",
        "                raise KeyError(\"Could not find expected data structure in electron file\")\n",
        "\n",
        "    # Load photon data (class 1)\n",
        "    with h5py.File(photon_file, 'r') as f:\n",
        "        print(\"Photon file structure:\")\n",
        "        print_hdf5_structure(f)\n",
        "        try:\n",
        "            photon_energy = np.array(f['energy'])\n",
        "            photon_time = np.array(f['time'])\n",
        "        except KeyError:\n",
        "            # If those keys don't exist, look for other possible structures\n",
        "            if 'X' in f:\n",
        "                data = np.array(f['X'])\n",
        "                print(f\"Found data with shape: {data.shape}\")\n",
        "\n",
        "                if len(data.shape) == 4:\n",
        "                    if data.shape[1] == 2:  # (n_samples, 2, height, width)\n",
        "                        photon_energy = data[:, 0]\n",
        "                        photon_time = data[:, 1]\n",
        "                    elif data.shape[3] == 2:  # (n_samples, height, width, 2)\n",
        "                        photon_energy = data[..., 0]\n",
        "                        photon_time = data[..., 1]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected data dimensions: {len(data.shape)}\")\n",
        "            else:\n",
        "                print(\"Available keys in photon file:\", list(f.keys()))\n",
        "                raise KeyError(\"Could not find expected data structure in photon file\")\n",
        "\n",
        "    # Print shapes to verify data loading\n",
        "    print(f\"Electron energy shape: {electron_energy.shape}\")\n",
        "    print(f\"Electron time shape: {electron_time.shape}\")\n",
        "    print(f\"Photon energy shape: {photon_energy.shape}\")\n",
        "    print(f\"Photon time shape: {photon_time.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rGEfBD2GbXnC",
      "metadata": {
        "id": "rGEfBD2GbXnC"
      },
      "outputs": [],
      "source": [
        "# data_structure(electron_path, photon_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "yob5nCiLbXsO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yob5nCiLbXsO",
        "outputId": "5bbce003-b704-4dad-d7a0-a64e0b938ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'X' shape: (249000, 32, 32, 2)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABdIAAAFECAYAAAAjhszqAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIyZJREFUeJzt3XuUVeV9N/DfgWFgwIIyCl4QEEQjXhGElGghpqigUUwExRtal29TbbSxNTExCYm5VAUv1VqSaIJQoelSUat5LWKCCdWUhfEWmvqKBCJao1y8oAjMMM/7B53R4cw854ADMwyfz1qs5ezzm72fs2fm65nv2XNOIaWUAgAAAAAAaFKH1l4AAAAAAAC0ZYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdLbJypUro0uXLvHkk0+29lJaxBNPPBGFQiGeeOKJnX7sc845JyZOnLjTjwvtmYxqOddcc02MGDFipx8X2jMZ1XJkFLQ8GdVyZBS0PBnVcvRR20+Rvg3uvvvuKBQKzf77z//8z9Ze4g533XXXxYgRI+JTn/pUo+0PP/xwjBo1Knr16hVdu3aNAQMGxMSJE+Pf//3fW2mlrevHP/5xHHbYYdGlS5cYNGhQ3H777UUzX/nKV+L++++P559/vhVWSHsko2RUOaZPnx4TJkyIvn37RqFQiIsuuqjJub/5m7+J559/Pv7t3/5t5y6QdktGyahSVq5cGd/+9rdj+PDhsddee8Xee+8do0ePjscff7xoVkbR0mSUjCrlgw8+iEsuuSSOOOKI6NGjR+yxxx5x9NFHxz/8wz9ETU1No1kZRUuTUTJqW/3Hf/xHw/fH6tWrG92mj9p+Fa29gF3RddddFwcddFDR9oMPPrgVVrPzrFq1KmbOnBkzZ85stH3atGlx9dVXx6hRo+KrX/1qdO3aNV5++eV4/PHH46c//WmccsoprbTi1vHDH/4wvvCFL8TnP//5uOqqq2LhwoVxxRVXxPr16+MrX/lKw9yQIUNi2LBhcdNNN8WsWbNaccW0NzJKRuXccMMNsW7duhg+fHi8/vrrzc7tu+++ccYZZ8S0adPi9NNP34krpL2TUTKqOQ899FDccMMNMX78+Jg8eXLU1tbGrFmzYsyYMfGTn/wkLr744oZZGcWOIqNkVHM++OCD+K//+q8YN25c9O/fPzp06BBPPfVUfOlLX4pFixbFnDlzGmZlFDuKjJJR5airq4svfvGL0a1bt3j//feLbtdHfQyJss2YMSNFRFq8eHFrLyWllNJ77723U4938803p6qqqrRu3bqGbTU1Nal79+5pzJgxTX7OG2+8sbOWt10WLFiQIiItWLCgRfa3fv36VF1dnU499dRG288777zUrVu3tHbt2kbbp02blrp169bonML2klEyqhwrVqxIdXV1KaWUunXrliZPntzs7H333ZcKhUJatmxZix2f3ZeMklGlLFmyJK1atarRtg0bNqRPfOITqU+fPkXzMoqWJKNk1Pb667/+6xQR6fXXX2+0XUbRkmSUjNoW06dPT9XV1enKK69MEVH0+ColfdT28tIuO8CKFSuiUCjEtGnT4kc/+lEMHDgwOnfuHMcdd1wsXry4aP7FF1+Ms846K3r27BldunSJYcOGFf0JWP2f8fzyl7+Myy67LHr16hV9+vRpuP2OO+6IAQMGRFVVVQwfPjwWLlwYo0ePjtGjR0dExHvvvRfdunWLK6+8suj4r776anTs2DH+/u//Pnu/HnzwwRgxYkTsscceDdtWr14d7777btGf1tTr1atXw39v2rQpvvnNb8bQoUOjR48e0a1btzjhhBNiwYIFzZ6/+vvVtWvXOOmkk2LlypWRUorvfOc70adPn6iqqoozzjgj1q5d22gf/fv3j9NOOy0ee+yxOOaYY6JLly4xePDgmDt3bvY+1lu0aFGccsop0aNHj+jatWuMGjWqrNfhWrBgQaxZsyYuu+yyRtsvv/zyeP/99+NnP/tZo+1jxoyJ999/P+bPn1/WuqAlyKgP7W4ZFRHRr1+/KBQKZc3++Z//eURsuUoUdhYZ9aHdLaMOP/zw2HvvvRtt69y5c4wbNy5effXVWLduXaPbZBStQUZ9aHfLqOb0798/IiLefvvtRttlFK1BRn1od82otWvXxte//vW47rrrYs8992x2Th+1nVq1xt/F1D8D+Pjjj6dVq1Y1+rd69eqGueXLl6eISEOGDEkHH3xwuuGGG9KNN96Y9t5779SnT5+0adOmhtklS5akHj16pMGDB6cbbrgh/eM//mP6sz/7s1QoFNLcuXOLjj148OA0atSodPvtt6frr78+pZTSP/3TP6WISCeccEK67bbb0lVXXZV69uyZBg4cmEaNGtWwj/POOy/17t071dbWNrpfN954YyoUCukPf/hDs/d906ZNqaqqKl111VWNtm/evDlVVVWloUOHpjVr1mTP36pVq9J+++2XrrrqqjR9+vR04403pkMPPTR16tQpPfvss0Xn75hjjkmDBw9ON998c/r617+eKisr0yc/+cn0ta99LY0cOTLddttt6YorrkiFQiFdfPHFjY7Vr1+/dMghh6Q999wzXXPNNenmm29ORx55ZOrQoUN67LHHGuaaegbw5z//eaqsrEx/+qd/mm666aZ0yy23pKOOOipVVlamRYsWZe/jd7/73RQRRc98bty4MXXo0KHo/NXU1KSqqqr0t3/7t9n9QjlklIwqlVFbK3VFekopHXzwwenzn//8Nu0XmiKjZNS2ZlS9c889N3Xt2rXo3Kcko2g5MkpGlZtRGzduTKtWrUqvvPJKmjt3btp3331Tv379Uk1NTdGsjKKlyCgZVW5GXXbZZenwww9PtbW1acqUKc1eka6P2j6K9G1QHx5N/evcuXPDXP0PXnV1daOX8njooYdSRKSHH364YdtnPvOZdOSRR6YNGzY0bKurq0sjR45MgwYNKjr28ccf3yh4Nm7cmKqrq9Nxxx3X6H/cd999d4qIRsE1b968FBHp0UcfbXS/jjrqqEZzTXn55ZdTRKTbb7+96LZvfvObKSJSt27d0tixY9P3vve99Jvf/KZorra2Nm3cuLHRtrfeeiv17t07/cVf/EXDtvrzt88++6S33367YftXv/rVFBHp6KOPbnRfJ02alCorKxudw379+qWISPfff3/DtnfeeSftt99+aciQIQ3btg6uurq6NGjQoHTyySc3vPRBSltesuWggw5q9k+G6l1++eWpY8eOTd62zz77pHPOOado+yGHHJLGjh2b3S+UQ0bJqFIZtbVyivSTTjopHXbYYdu0X2iKjJJR25pRKaW0dOnS1KVLl3TBBRc0ebuMoqXIKBlVbkb9y7/8S6Pvj2HDhqUXXnihyVkZRUuRUTKqnIx6/vnnU8eOHdO8efNSSilbpKekj9oeXtplO9xxxx0xf/78Rv8effTRormzzz479tprr4aPTzjhhIiI+P3vfx8RW/7c4he/+EVMnDgx1q1bF6tXr47Vq1fHmjVr4uSTT46lS5fGa6+91mifl156aXTs2LHh46effjrWrFkTl156aVRUfPjeseedd16jY0ds+dOy/fffP2bPnt2wbcmSJfHCCy/E+eefn73Pa9asiYgo2mdExLe//e2YM2dODBkyJObNmxfXXnttDB06NI499tj47//+74a5jh07RmVlZURseeODtWvXRm1tbQwbNiyeeeaZov1OmDAhevTo0fDxiBEjIiLi/PPPb3RfR4wYEZs2bSo6V/vvv3+ceeaZDR937949Lrzwwnj22Wfjj3/8Y5P387nnnoulS5fGueeeG2vWrGn4mrz//vvxmc98Jn71q19FXV1ds+fpgw8+aLiPW+vSpUt88MEHRdv32muvondQho9DRjUmoz4eGUVLk1GNyajmrV+/PiZMmBBVVVVx/fXXNzkjo2hpMqoxGVXs05/+dMyfPz/uvffe+MIXvhCdOnVq8s38ImQULU9GNSajGrviiiti7NixcdJJJ2Xn6smobVdReoStDR8+PIYNG1Zyrm/fvo0+rv+hf+uttyIi4uWXX46UUnzjG9+Ib3zjG03u480334wDDjig4eOt3535D3/4Q0QUv0NzRUVFw2u11evQoUOcd955MX369Fi/fn107do1Zs+eHV26dIkJEyaUvD8RESmlJrdPmjQpJk2aFO+++24sWrQo7r777pgzZ0589rOfjSVLlkSXLl0iImLmzJlx0003xYsvvhg1NTXN3q+I4vNXH2IHHnhgk9vrz2u9gw8+uOh1gA855JCI2PK6V/vuu2/RMZcuXRoREZMnT27yfkZEvPPOO00GeEREVVVVbNq0qcnbNmzYEFVVVUXbU0plv14xlENGFZNR209G0dJkVDEZVWzz5s1xzjnnxO9+97t49NFHY//9929yTkbR0mRUMRnVWO/evaN3794REXHWWWfF97///RgzZkwsXbq06LgyipYmo4rJqC3+9V//NZ566qlYsmRJs5+/NRm17RTpO9BHn6n7qPof/vpnkv7u7/4uTj755CZntw6kporYbXHhhRfG1KlT48EHH4xJkybFnDlz4rTTTmv0TFtTqqurI6I4HLbWvXv3GDNmTIwZMyY6deoUM2fOjEWLFsWoUaPinnvuiYsuuijGjx8fV199dfTq1avhTSWWLVtWtK/mzl+p8/px1H9Npk6dGsccc0yTMx99c4ut7bfffrF58+Z48803i97YYs2aNU3+EvjWW2/FoEGDPt7CYTvIqN0vo7bHW2+9VfTmf7AzyKjdO6MuvfTSeOSRR2L27Nlx4oknNjsno2gtMmr3zqiPOuuss+Laa6+Nhx56KP7yL/+y0W0yitYio3a/jLr66qtjwoQJUVlZGStWrIiID98EeeXKlbFp06aiTkofte0U6a1owIABERHRqVOnhnf03lb9+vWLiC3PJn76059u2F5bWxsrVqyIo446qtH8EUccEUOGDInZs2dHnz594pVXXonbb7+95HH69u0bVVVVsXz58rLXNmzYsJg5c2a8/vrrERFx3333xYABA2Lu3LmNnvGaMmVK2fvcFvXPsH70WC+99FJERNGzo/UGDhwYEVsCeHu+JvVh9/TTT8e4ceMatj/99NNRV1dXFIa1tbWxcuXKOP3007f5WLCjyagt2lNGbY/ly5fH0UcfvVOOBdtCRm3RHjPq6quvjhkzZsStt94akyZNys7KKNoqGbVFe8yordW/fOc777xTdJuMoq2SUVu0p4xauXJlzJkzJ+bMmVN027HHHhtHH310PPfccw3b9FHbx2ukt6JevXrF6NGj44c//GHDD/dHrVq1quQ+hg0bFtXV1XHnnXdGbW1tw/bZs2c3+2zdBRdcEI899ljceuutUV1dHWPHji15nE6dOsWwYcPi6aefbrR9/fr18etf/7rJz6l/na5DDz00Ij585u6jz9QtWrSo2c//uP7nf/4nHnjggYaP33333Zg1a1Ycc8wxTf4ZTUTE0KFDY+DAgTFt2rR47733im4v9TU58cQTo2fPnjF9+vRG26dPnx5du3aNU089tdH23/3ud7Fhw4YYOXJkuXcLdhoZ1f4yalu98847sWzZMhlFmySj2mdGTZ06NaZNmxZf+9rX4sorr8zOyijaMhnV/jJq9erVTV51etddd0VEFL3choyiLZNR7S+jHnjggaJ/Z599dkREzJo1K2655ZZG8/qo7eOK9O3w6KOPxosvvli0feTIkQ3P6pXrjjvuiOOPPz6OPPLIuPTSS2PAgAHxxhtvxK9//et49dVX4/nnn89+fmVlZXzrW9+KL37xi3HiiSfGxIkTY8WKFXH33XfHwIEDm3yto3PPPTe+/OUvxwMPPBB/9Vd/FZ06dSprrWeccUZce+218e6770b37t0jYktwjRw5Mj75yU/GKaecEgceeGC8/fbb8eCDD8bChQtj/PjxMWTIkIiIOO2002Lu3Llx5plnxqmnnhrLly+PH/zgBzF48OAmQ+LjOuSQQ+KSSy6JxYsXR+/eveMnP/lJvPHGGzFjxoxmP6dDhw5x1113xdixY+Pwww+Piy++OA444IB47bXXYsGCBdG9e/d4+OGHm/38qqqq+M53vhOXX355TJgwIU4++eRYuHBh3HPPPfG9730vevbs2Wh+/vz50bVr1xgzZkyL3W+QUTIq5+GHH274utXU1MQLL7wQ3/3udyMi4vTTT2905cjjjz8eKaU444wzWuAewxYySkY154EHHogvf/nLMWjQoDjssMPinnvuaXT7mDFjGl6XOEJGsWPIKBnVnHvuuSd+8IMfxPjx42PAgAGxbt26mDdvXsyfPz8++9nPFr0MlYxiR5BRMqo548ePL9pWfwX62LFji15mSh+1nRJlmzFjRoqIZv/NmDEjpZTS8uXLU0SkqVOnFu0jItKUKVMabVu2bFm68MIL07777ps6deqUDjjggHTaaael++67r+jYixcvbnJtt912W+rXr1/q3LlzGj58eHryySfT0KFD0ymnnNLk/Lhx41JEpKeeeqrs+//GG2+kioqK9M///M8N22pqatKdd96Zxo8f33D8rl27piFDhqSpU6emjRs3NszW1dWl73//+w1zQ4YMSY888kiaPHly6tevX8Ncc+dvwYIFKSLSvffe22h7U+emX79+6dRTT03z5s1LRx11VOrcuXP6xCc+UfS59ftcsGBBo+3PPvts+tznPpeqq6tT586dU79+/dLEiRPTz3/+87LO1Y9+9KN06KGHpsrKyjRw4MB0yy23pLq6uqK5ESNGpPPPP7+sfUIpMkpGlZNRkydPLvk9Uu/ss89Oxx9/fMl9QjlklIwqlVFTpkzJfo9sfRwZRUuSUTKqVEYtXrw4TZgwIfXt2zd17tw5devWLR177LHp5ptvTjU1NUXzMoqWJKNkVLl91EfVP7ZatWpV0W36qO1TSKkFXhGfNqeuri722Wef+NznPhd33nln0e1nnnlm/Pa3v42XX355m/Z7ySWXxEsvvRQLFy5sqaXuEP37948jjjgiHnnkkdZeSrOee+65OPbYY+OZZ55p9o0koL2SUW0/o/74xz/GQQcdFD/96U9dScVuR0bJKGjLZJSMgrZMRrX9jNJHbT+vkd4ObNiwoei12mbNmhVr166N0aNHF82//vrr8bOf/SwuuOCCbT7WlClTYvHixfHkk09u73L5X9dff32cddZZQot2T0btmm699dY48sgj/fJHuyejdk0yit2FjNo1ySh2FzJq16SP2n6uSG8HnnjiifjSl74UEyZMiOrq6njmmWfixz/+cRx22GHxm9/8JiorKyNiyzuGP/nkk3HXXXfF4sWLY9myZc2+ycGubld4BhB2FzKqmIyCtkNGFZNR0HbIqGIyCtoOGVVMRrVv3my0Hejfv38ceOCBcdttt8XatWujZ8+eceGFF8b111/fEFoREb/85S/j4osvjr59+8bMmTPbbWgBbYuMAtoyGQW0ZTIKaMtkFLsbV6QDAAAAAECG10gHAAAAAIAMRToAAAAAAGQo0gEAAAAAIKPsNxsd02HCjlwHQMyvu3e7P1dGATuajALaMhkFtGUyCmjLys0oV6QDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZFS09gIAAAAAANqdQqH0TEo7fh20CFekAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZFa29AAAAYAcpFPK3p7Rz1hFRei0RO3c9AAA7msc27Yor0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkVLT2AgAAgB0kpdZewYfa0loAAGAbuSIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQEZFay8AAADYxRUKpWdS2vHrAHY/8geAncQV6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyKlp7AQAA0CYUCqVnUtrx69gVOS+0FD+HbKvd9fvBzwrATueKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZFa29AAAAaBNSau0VAH4Odw2FQukZX8sdy/kF2OlckQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgo6K1FwAAAOwghUL+9pR2zjqA9kV2AG1Zqcc/5ZJ1bMUV6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABARkVrLwAAAHYrhULpkYpOLXKotHlzqYEWOQ7QRpTKl5R2znFa8liwu9pZP887Uwvcp0JFGVVmoWWuG061NS2wk13w60SzXJEOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIKOitRfQ1hUq8qco1daWsZNC6ZmUylxR21DqvJQ7U875K+scA7uGnZWHZRyn0LFj6aVs3lz6WLtYfgMfUzk5VkKHPfYoOfP7a44oObNp/00lZw79P7/N3p5SXcl9lEUWwo7XQo9vSu6jnN/1KitLztStX19ypuRjrZbKlhbI7oiQdexcu+H3W8fu3UsP7btPyZHNe3YtOVPx+lul9/PmquztaePGkvtoj53g7swV6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyKlp7AW1dqq1tgZ2k0jOFQsvspwUUKkp/W2w4aUjJmSfuurPkzLjBo0rObH73veztFX0PKLmP2ldeKzkTdZtLzwAfT0vkWBl52fFP/qTkzP998VclZ8YdPabkzOY1a/MDsgXal1I5Vs5jujL8v4unl5w57pmJJWcKHfPXzaRyHurupMegQF6hY8eSM2suPC57e68nXi+5j9pe3UvOzJs7q+TM2EGfKjmT1q8vOVNKoVNlyZkOBx1YcmbzS8s+9lqAEgolHpeU8Zjj0SfuLzlz8v7HlJyp7VA6U0tqiX1ERJTz8LGc3ytLPQ71mO5jc0U6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgIyK1l4A/yul1l5Bg7R5c8mZql/8tuTM2ENPKH2sTRvKWFBd9ubaFa+U3gewW0m1tSVnTppwUcmZjhuWlXGwfEYBbK3uvfdKzpxy0IiSM9V1K0ofq7YmP9CGHoMCeeU8vqmetTh7e+nf9CIKK0tfbzf24JElZ1JNifxpIalmU8mZzS+V8ZgOaF6hUHqmnMcUdfkUqnt/fcldlJM/Hf6kY+mllHGsXY7HdTucK9IBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZFS09gJog1IqOVK3cWPp/WzY0AKLAdhKORm1fn3JmcJTz5feT1kLAviIMjIqCoXSu6mpLb2fus0tciygDSjnZ7WMfEm1ZWRHC0i1NWUMlZGHwK5hZ/08l/HYppzf9crSoWPL7IfdiivSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAICMitZeALuolFp7BQAfjxwDWks5+ZM277xjAWwr2dL6CoXSM75OAC3KFekAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMipaewEAAADQ5hQKpWdSapljlbOfnbke2j5fa2heOXkJ28EV6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyKlp7AQDQKgqF0jMp7fh1AABtU1t7HNDW1gPQVpWTl2nzxz+O3yl3O65IBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJBR0doLAIBWkVJrrwAAAIBdVTm/UxYKLbMf2gRXpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIqGjtBQAAAAAAtDsptfYKaEGuSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMhTpAAAAAACQoUgHAAAAAIAMRToAAAAAAGQo0gEAAAAAIEORDgAAAAAAGYp0AAAAAADIUKQDAAAAAECGIh0AAAAAADIU6QAAAAAAkKFIBwAAAACADEU6AAAAAABkKNIBAAAAACBDkQ4AAAAAABmKdAAAAAAAyFCkAwAAAABAhiIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQEYhpZRaexEAAAAAANBWuSIdAAAAAAAyFOkAAAAAAJChSAcAAAAAgAxFOgAAAAAAZCjSAQAAAAAgQ5EOAAAAAAAZinQAAAAAAMhQpAMAAAAAQIYiHQAAAAAAMv4/ukPoDCJO+HsAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'X' shape: (249000, 32, 32, 2)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def visualize_data(file_path, dataset_name='X', num_images=5):\n",
        "    \"\"\"\n",
        "    Visualizes a specified dataset from an HDF5 file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the HDF5 file.\n",
        "        dataset_name (str): Name of the dataset to visualize.\n",
        "        num_images (int): Number of images to display.\n",
        "    \"\"\"\n",
        "\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        if dataset_name in f:\n",
        "            data = np.array(f[dataset_name])\n",
        "            print(f\"Dataset '{dataset_name}' shape: {data.shape}\")\n",
        "\n",
        "            if len(data.shape) == 4 and data.shape[1] == 2:\n",
        "                fig, axes = plt.subplots(1, min(num_images, data.shape[0]), figsize=(15, 5))\n",
        "                for i in range(min(num_images, data.shape[0])):\n",
        "                    energy = data[i, 0]\n",
        "                    time = data[i, 1]\n",
        "                    axes[i].imshow(energy, cmap='viridis')\n",
        "                    axes[i].set_title(f'Energy (Sample {i})')\n",
        "                    axes[i].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                fig, axes = plt.subplots(1, min(num_images, data.shape[0]), figsize=(15, 5))\n",
        "                for i in range(min(num_images, data.shape[0])):\n",
        "                    energy = data[i, 0]\n",
        "                    time = data[i, 1]\n",
        "                    axes[i].imshow(time, cmap='viridis')\n",
        "                    axes[i].set_title(f'Time (Sample {i})')\n",
        "                    axes[i].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "\n",
        "            elif len(data.shape) == 4 and data.shape[3] == 2:\n",
        "                fig, axes = plt.subplots(1, min(num_images, data.shape[0]), figsize=(15, 5))\n",
        "                for i in range(min(num_images, data.shape[0])):\n",
        "                    energy = data[i, :, :, 0]\n",
        "                    time = data[i, :, :, 1]\n",
        "                    axes[i].imshow(energy, cmap='viridis')\n",
        "                    axes[i].set_title(f'Energy (Sample {i})')\n",
        "                    axes[i].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                fig, axes = plt.subplots(1, min(num_images, data.shape[0]), figsize=(15, 5))\n",
        "                for i in range(min(num_images, data.shape[0])):\n",
        "                    energy = data[i, :, :, 0]\n",
        "                    time = data[i, :, :, 1]\n",
        "                    axes[i].imshow(time, cmap='viridis')\n",
        "                    axes[i].set_title(f'Time (Sample {i})')\n",
        "                    axes[i].axis('off')\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            else:\n",
        "                print(f\"Dataset '{dataset_name}' has an unsupported shape for visualization.\")\n",
        "        else:\n",
        "            print(f\"Dataset '{dataset_name}' not found in the file.\")\n",
        "\n",
        "visualize_data(electron_path)\n",
        "\n",
        "visualize_data(photon_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb1cdd0d-d4c3-467f-b841-95a3529330e7",
      "metadata": {
        "id": "eb1cdd0d-d4c3-467f-b841-95a3529330e7"
      },
      "source": [
        "Since there was problem while converting these images into array so I used 70% of the given data as 100% and then built the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753763f7-975a-4ff8-857a-3cee3926ac55",
      "metadata": {
        "id": "753763f7-975a-4ff8-857a-3cee3926ac55"
      },
      "outputs": [],
      "source": [
        "def load_particle_data(electron_file, photon_file,data_fraction=0.7, batch_processing=True):\n",
        "    \"\"\"\n",
        "    Load electron and photon data from HDF5 files.\n",
        "\n",
        "    Args:\n",
        "        electron_file (str): Path to electron data file\n",
        "        photon_file (str): Path to photon data file\n",
        "        batch_processing (bool): Whether to process data in batches to save memory\n",
        "        max_samples (int, optional): Maximum number of samples to load from each class\n",
        "\n",
        "    Returns:\n",
        "        tuple: (X_train, y_train, X_test, y_test)\n",
        "    \"\"\"\n",
        "    # Load electron data (class 0)\n",
        "    with h5py.File(electron_file, 'r') as f:\n",
        "        print(\"Electron file structure:\")\n",
        "        print_hdf5_structure(f)\n",
        "        try:\n",
        "            # First attempt with expected names\n",
        "            electron_energy = np.array(f['energy'])\n",
        "            electron_time = np.array(f['time'])\n",
        "        except KeyError:\n",
        "            # If those keys don't exist, look for other possible structures\n",
        "            if 'X' in f:\n",
        "                # If data is stored as a single dataset with multiple channels\n",
        "                data = np.array(f['X'])\n",
        "                print(f\"Found data with shape: {data.shape}\")\n",
        "\n",
        "                if len(data.shape) == 4:\n",
        "                    if data.shape[1] == 2:  # (n_samples, 2, height, width)\n",
        "                        electron_energy = data[:, 0]\n",
        "                        electron_time = data[:, 1]\n",
        "                    elif data.shape[3] == 2:  # (n_samples, height, width, 2)\n",
        "                        electron_energy = data[..., 0]\n",
        "                        electron_time = data[..., 1]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected data dimensions: {len(data.shape)}\")\n",
        "            else:\n",
        "                # List all available keys and try to infer which ones contain the data\n",
        "                print(\"Available keys in electron file:\", list(f.keys()))\n",
        "                raise KeyError(\"Could not find expected data structure in electron file\")\n",
        "\n",
        "    # Load photon data (class 1)\n",
        "    with h5py.File(photon_file, 'r') as f:\n",
        "        print(\"Photon file structure:\")\n",
        "        print_hdf5_structure(f)\n",
        "        try:\n",
        "            photon_energy = np.array(f['energy'])\n",
        "            photon_time = np.array(f['time'])\n",
        "        except KeyError:\n",
        "            # If those keys don't exist, look for other possible structures\n",
        "            if 'X' in f:\n",
        "                data = np.array(f['X'])\n",
        "                print(f\"Found data with shape: {data.shape}\")\n",
        "\n",
        "                if len(data.shape) == 4:\n",
        "                    if data.shape[1] == 2:  # (n_samples, 2, height, width)\n",
        "                        photon_energy = data[:, 0]\n",
        "                        photon_time = data[:, 1]\n",
        "                    elif data.shape[3] == 2:  # (n_samples, height, width, 2)\n",
        "                        photon_energy = data[..., 0]\n",
        "                        photon_time = data[..., 1]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unexpected data shape: {data.shape}\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected data dimensions: {len(data.shape)}\")\n",
        "            else:\n",
        "                print(\"Available keys in photon file:\", list(f.keys()))\n",
        "                raise KeyError(\"Could not find expected data structure in photon file\")\n",
        "\n",
        "    #Applying data fraction reduction\n",
        "    electron_total = len(electron_energy)\n",
        "    photon_total = len(photon_energy)\n",
        "\n",
        "    #Using data fraction\n",
        "    electron_samples = int(electron_total * data_fraction)\n",
        "    photon_samples = int(photon_total * data_fraction)\n",
        "\n",
        "    #Shuffle indices before selecting subset to ensure randomness\n",
        "    np.random.seed(42)\n",
        "    electron_indices = np.random.permutation(electron_total)[:electron_samples]\n",
        "    photon_indices = np.random.permutation(photon_total)[:photon_samples]\n",
        "\n",
        "    electron_energy = electron_energy[electron_indices]\n",
        "    electron_time = electron_time[electron_indices]\n",
        "    photon_energy = photon_energy[photon_indices]\n",
        "    photon_time = photon_time[photon_indices]\n",
        "\n",
        "\n",
        "    # Print shapes to verify data loading\n",
        "    print(f\"Using {data_fraction*100:.1f}% of original data\")\n",
        "    print(f\"Electron energy shape: {electron_energy.shape}\")\n",
        "    print(f\"Electron time shape: {electron_time.shape}\")\n",
        "    print(f\"Photon energy shape: {photon_energy.shape}\")\n",
        "    print(f\"Photon time shape: {photon_time.shape}\")\n",
        "\n",
        "    # Create labels\n",
        "    electron_labels = np.zeros(len(electron_energy))\n",
        "    photon_labels = np.ones(len(photon_energy))\n",
        "\n",
        "    if batch_processing:\n",
        "        # Process in memory-efficient batches\n",
        "        return _process_in_batches(electron_energy, electron_time, electron_labels,photon_energy, photon_time, photon_labels)\n",
        "    else:\n",
        "        # Combine data\n",
        "        energy = np.vstack((electron_energy, photon_energy))\n",
        "        time = np.vstack((electron_time, photon_time))\n",
        "        labels = np.hstack((electron_labels, photon_labels))\n",
        "\n",
        "        # Normalize data\n",
        "        energy = normalize_data(energy)\n",
        "        time = normalize_data(time)\n",
        "\n",
        "        # Stack channels to create 2-channel images\n",
        "        # Format: (samples, height, width, channels)\n",
        "        images = np.stack([energy, time], axis=-1)\n",
        "\n",
        "        # Split data into training and test sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            images, labels, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(f\"Dataset loaded: {len(labels)} samples\")\n",
        "        print(f\"Class distribution: {np.sum(labels == 0)} electrons, {np.sum(labels == 1)} photons\")\n",
        "        print(f\"Training set size: {len(X_train)}\")\n",
        "        print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "        return X_train, y_train, X_test, y_test\n",
        "\n",
        "def _process_in_batches(electron_energy, electron_time, electron_labels, photon_energy, photon_time, photon_labels, batch_size=4000):\n",
        "    \"\"\"\n",
        "    Process data in batches to avoid memory issues.\n",
        "\n",
        "    Args:\n",
        "        electron_energy, electron_time: Electron data arrays\n",
        "        electron_labels: Labels for electron data\n",
        "        photon_energy, photon_time: Photon data arrays\n",
        "        photon_labels: Labels for photon data\n",
        "        batch_size: Number of samples to process at once\n",
        "\n",
        "    Returns:\n",
        "        X_train, y_train, X_test, y_test: Train/test split data\n",
        "    \"\"\"\n",
        "    # Initialize empty lists for train/test data\n",
        "    X_train_list = []\n",
        "    X_test_list = []\n",
        "    y_train_list = []\n",
        "    y_test_list = []\n",
        "\n",
        "    # Set random seed for consistent splits\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Calculate total samples\n",
        "    total_electron = len(electron_energy)\n",
        "    total_photon = len(photon_energy)\n",
        "    total_samples = total_electron + total_photon\n",
        "\n",
        "    print(f\"Processing {total_samples} samples in batches of {batch_size}\")\n",
        "\n",
        "    # Process electron data in batches\n",
        "    for start_idx in range(0, total_electron, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_electron)\n",
        "        batch_size_actual = end_idx - start_idx\n",
        "\n",
        "        # Get batch data\n",
        "        e_energy_batch = normalize_data(electron_energy[start_idx:end_idx])\n",
        "        e_time_batch = normalize_data(electron_time[start_idx:end_idx])\n",
        "        e_labels_batch = electron_labels[start_idx:end_idx]\n",
        "\n",
        "        # Stack channels\n",
        "        e_images_batch = np.stack([e_energy_batch, e_time_batch], axis=-1)\n",
        "\n",
        "        # Determine train/test split (80/20)\n",
        "        split_idx = int(0.8 * batch_size_actual)\n",
        "        indices = np.random.permutation(batch_size_actual)\n",
        "        train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
        "\n",
        "        # Add to train/test lists\n",
        "        X_train_list.append(e_images_batch[train_idx])\n",
        "        X_test_list.append(e_images_batch[test_idx])\n",
        "        y_train_list.append(e_labels_batch[train_idx])\n",
        "        y_test_list.append(e_labels_batch[test_idx])\n",
        "\n",
        "        print(f\"Processed electron batch {start_idx//batch_size + 1}/{(total_electron-1)//batch_size + 1}\")\n",
        "\n",
        "    # Process photon data in batches\n",
        "    for start_idx in range(0, total_photon, batch_size):\n",
        "        end_idx = min(start_idx + batch_size, total_photon)\n",
        "        batch_size_actual = end_idx - start_idx\n",
        "\n",
        "        # Get batch data\n",
        "        p_energy_batch = normalize_data(photon_energy[start_idx:end_idx])\n",
        "        p_time_batch = normalize_data(photon_time[start_idx:end_idx])\n",
        "        p_labels_batch = photon_labels[start_idx:end_idx]\n",
        "\n",
        "        # Stack channels\n",
        "        p_images_batch = np.stack([p_energy_batch, p_time_batch], axis=-1)\n",
        "\n",
        "        # Determine train/test split (80/20)\n",
        "        split_idx = int(0.8 * batch_size_actual)\n",
        "        indices = np.random.permutation(batch_size_actual)\n",
        "        train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
        "\n",
        "        # Add to train/test lists\n",
        "        X_train_list.append(p_images_batch[train_idx])\n",
        "        X_test_list.append(p_images_batch[test_idx])\n",
        "        y_train_list.append(p_labels_batch[train_idx])\n",
        "        y_test_list.append(p_labels_batch[test_idx])\n",
        "\n",
        "        print(f\"Processed photon batch {start_idx//batch_size + 1}/{(total_photon-1)//batch_size + 1}\")\n",
        "\n",
        "    # Concatenate results\n",
        "    X_train = np.concatenate(X_train_list)\n",
        "    X_test = np.concatenate(X_test_list)\n",
        "    y_train = np.concatenate(y_train_list)\n",
        "    y_test = np.concatenate(y_test_list)\n",
        "\n",
        "\n",
        "    print(f\"Dataset loaded: {total_samples} samples\")\n",
        "    print(f\"Class distribution: {total_electron} electrons, {total_photon} photons\")\n",
        "    print(f\"Training set size: {len(X_train)}\")\n",
        "    print(f\"Test set size: {len(X_test)}\")\n",
        "\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "mR7BnAncbYBG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR7BnAncbYBG",
        "outputId": "540c1397-a053-4191-b73e-75bc29001b7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Electron file structure:\n",
            "- Dataset: X, Shape: (249000, 32, 32, 2), Dtype: float32\n",
            "- Dataset: y, Shape: (249000,), Dtype: float32\n",
            "Found data with shape: (249000, 32, 32, 2)\n",
            "Photon file structure:\n",
            "- Dataset: X, Shape: (249000, 32, 32, 2), Dtype: float32\n",
            "- Dataset: y, Shape: (249000,), Dtype: float32\n",
            "Found data with shape: (249000, 32, 32, 2)\n",
            "Using 70.0% of original data\n",
            "Electron energy shape: (174300, 32, 32)\n",
            "Electron time shape: (174300, 32, 32)\n",
            "Photon energy shape: (174300, 32, 32)\n",
            "Photon time shape: (174300, 32, 32)\n",
            "Processing 348600 samples in batches of 4000\n",
            "Processed electron batch 1/44\n",
            "Processed electron batch 2/44\n",
            "Processed electron batch 3/44\n",
            "Processed electron batch 4/44\n",
            "Processed electron batch 5/44\n",
            "Processed electron batch 6/44\n",
            "Processed electron batch 7/44\n",
            "Processed electron batch 8/44\n",
            "Processed electron batch 9/44\n",
            "Processed electron batch 10/44\n",
            "Processed electron batch 11/44\n",
            "Processed electron batch 12/44\n",
            "Processed electron batch 13/44\n",
            "Processed electron batch 14/44\n",
            "Processed electron batch 15/44\n",
            "Processed electron batch 16/44\n",
            "Processed electron batch 17/44\n",
            "Processed electron batch 18/44\n",
            "Processed electron batch 19/44\n",
            "Processed electron batch 20/44\n",
            "Processed electron batch 21/44\n",
            "Processed electron batch 22/44\n",
            "Processed electron batch 23/44\n",
            "Processed electron batch 24/44\n",
            "Processed electron batch 25/44\n",
            "Processed electron batch 26/44\n",
            "Processed electron batch 27/44\n",
            "Processed electron batch 28/44\n",
            "Processed electron batch 29/44\n",
            "Processed electron batch 30/44\n",
            "Processed electron batch 31/44\n",
            "Processed electron batch 32/44\n",
            "Processed electron batch 33/44\n",
            "Processed electron batch 34/44\n",
            "Processed electron batch 35/44\n",
            "Processed electron batch 36/44\n",
            "Processed electron batch 37/44\n",
            "Processed electron batch 38/44\n",
            "Processed electron batch 39/44\n",
            "Processed electron batch 40/44\n",
            "Processed electron batch 41/44\n",
            "Processed electron batch 42/44\n",
            "Processed electron batch 43/44\n",
            "Processed electron batch 44/44\n",
            "Processed photon batch 1/44\n",
            "Processed photon batch 2/44\n",
            "Processed photon batch 3/44\n",
            "Processed photon batch 4/44\n",
            "Processed photon batch 5/44\n",
            "Processed photon batch 6/44\n",
            "Processed photon batch 7/44\n",
            "Processed photon batch 8/44\n",
            "Processed photon batch 9/44\n",
            "Processed photon batch 10/44\n",
            "Processed photon batch 11/44\n",
            "Processed photon batch 12/44\n",
            "Processed photon batch 13/44\n",
            "Processed photon batch 14/44\n",
            "Processed photon batch 15/44\n",
            "Processed photon batch 16/44\n",
            "Processed photon batch 17/44\n",
            "Processed photon batch 18/44\n",
            "Processed photon batch 19/44\n",
            "Processed photon batch 20/44\n",
            "Processed photon batch 21/44\n",
            "Processed photon batch 22/44\n",
            "Processed photon batch 23/44\n",
            "Processed photon batch 24/44\n",
            "Processed photon batch 25/44\n",
            "Processed photon batch 26/44\n",
            "Processed photon batch 27/44\n",
            "Processed photon batch 28/44\n",
            "Processed photon batch 29/44\n",
            "Processed photon batch 30/44\n",
            "Processed photon batch 31/44\n",
            "Processed photon batch 32/44\n",
            "Processed photon batch 33/44\n",
            "Processed photon batch 34/44\n",
            "Processed photon batch 35/44\n",
            "Processed photon batch 36/44\n",
            "Processed photon batch 37/44\n",
            "Processed photon batch 38/44\n",
            "Processed photon batch 39/44\n",
            "Processed photon batch 40/44\n",
            "Processed photon batch 41/44\n",
            "Processed photon batch 42/44\n",
            "Processed photon batch 43/44\n",
            "Processed photon batch 44/44\n",
            "Dataset loaded: 348600 samples\n",
            "Class distribution: 174300 electrons, 174300 photons\n",
            "Training set size: 278880\n",
            "Test set size: 69720\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_test, y_test = load_particle_data(electron_path, photon_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "jQQGbkmebYDz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQQGbkmebYDz",
        "outputId": "b52cb924-81c0-4232-d721-a046efccdd7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((278880, 32, 32, 2), (278880,))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d0d040e4-5c77-4a1d-8375-cea14377a44c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0d040e4-5c77-4a1d-8375-cea14377a44c",
        "outputId": "0fd7e863-2dfa-4ffb-b7be-eef455a76645"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((69720, 32, 32, 2), (69720,))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7d91f109-0fe0-43ca-adfb-73afe689202d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d91f109-0fe0-43ca-adfb-73afe689202d",
        "outputId": "639a9732-dd48-44bf-b021-19e62b5f58ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train dtype: float32\n",
            "y_train dtype: float64\n",
            "X_test dtype: float32\n",
            "y_test dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train dtype:\", X_train.dtype)\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "print(\"X_test dtype:\", X_test.dtype)\n",
        "print(\"y_test dtype:\", y_test.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d4e2f3a8-ba00-437c-9d0a-a154e69fade8",
      "metadata": {
        "id": "d4e2f3a8-ba00-437c-9d0a-a154e69fade8"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.astype(\"int32\")\n",
        "y_test = y_test.astype(\"int32\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "8c08af1f-314b-4a0e-874a-f1fc06e1c567",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c08af1f-314b-4a0e-874a-f1fc06e1c567",
        "outputId": "205812ea-b88a-4c12-bdeb-ed9cced7cb87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train dtype: float32\n",
            "y_train dtype: int32\n",
            "X_test dtype: float32\n",
            "y_test dtype: int32\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train dtype:\", X_train.dtype)\n",
        "print(\"y_train dtype:\", y_train.dtype)\n",
        "print(\"X_test dtype:\", X_test.dtype)\n",
        "print(\"y_test dtype:\", y_test.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "dbd59f6d-73c9-4420-8d29-de9f940a65ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbd59f6d-73c9-4420-8d29-de9f940a65ce",
        "outputId": "1d04f62f-25fd-4797-b8a1-25fe1b04d041"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NaN in X_train: 0\n",
            "Inf in X_train: 0\n"
          ]
        }
      ],
      "source": [
        "print(\"NaN in X_train:\", np.isnan(X_train).sum())\n",
        "print(\"Inf in X_train:\", np.isinf(X_train).sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c0b24a2a-54ca-4eeb-ae18-8c4b91442794",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0b24a2a-54ca-4eeb-ae18-8c4b91442794",
        "outputId": "33005e07-6bbe-4c87-cd6b-21080c54f528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "278880"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6981afbd-01ac-4fc0-8deb-f1d90f595f83",
      "metadata": {
        "id": "6981afbd-01ac-4fc0-8deb-f1d90f595f83"
      },
      "outputs": [],
      "source": [
        "# Basic Block function for ResNet\n",
        "def basic_block(x, filters, stride=1, name=None):\n",
        "    \"\"\"\n",
        "    Create a basic ResNet block with two conv layers and a skip connection.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor\n",
        "        filters: Number of filters for the convolutional layers\n",
        "        stride: Stride for the first convolutional layer\n",
        "        name: Prefix for the layers' names\n",
        "\n",
        "    Returns:\n",
        "        Output tensor after applying the basic block\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = 'basic_block'\n",
        "\n",
        "    # First convolution\n",
        "    conv1 = layers.Conv2D(filters, kernel_size=3, strides=stride, padding='same',use_bias=False, name=f'{name}_conv1')(x)\n",
        "    bn1 = layers.BatchNormalization(name=f'{name}_bn1')(conv1)\n",
        "    relu1 = layers.ReLU(name=f'{name}_relu1')(bn1)\n",
        "\n",
        "    # Second convolution\n",
        "    conv2 = layers.Conv2D(filters, kernel_size=3, strides=1, padding='same', use_bias=False, name=f'{name}_conv2')(relu1)\n",
        "    bn2 = layers.BatchNormalization(name=f'{name}_bn2')(conv2)\n",
        "\n",
        "    # Skip connection (shortcut)\n",
        "    if stride != 1 or x.shape[-1] != filters:\n",
        "        shortcut = layers.Conv2D(filters, kernel_size=1, strides=stride,use_bias=False, name=f'{name}_shortcut_conv')(x)\n",
        "        shortcut = layers.BatchNormalization(name=f'{name}_shortcut_bn')(shortcut)\n",
        "    else:\n",
        "        shortcut = x\n",
        "\n",
        "    # Add skip connection and apply ReLU\n",
        "    out = layers.add([bn2, shortcut], name=f'{name}_add')\n",
        "    out = layers.ReLU(name=f'{name}_relu2')(out)\n",
        "\n",
        "    return out\n",
        "\n",
        "# Create a layer of basic blocks\n",
        "def resnet_layer(x, blocks, filters, stride=1, name=None):\n",
        "    \"\"\"\n",
        "    Create a layer of multiple basic blocks.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor\n",
        "        blocks: Number of basic blocks in the layer\n",
        "        filters: Number of filters for the convolutional layers\n",
        "        stride: Stride for the first block in the layer\n",
        "        name: Prefix for the layer's name\n",
        "\n",
        "    Returns:\n",
        "        Output tensor after applying the layer\n",
        "    \"\"\"\n",
        "    if name is None:\n",
        "        name = 'layer'\n",
        "\n",
        "    # First block may have a stride != 1\n",
        "    x = basic_block(x, filters, stride=stride, name=f'{name}_block1')\n",
        "\n",
        "    # Remaining blocks maintain dimensions\n",
        "    for i in range(1, blocks):\n",
        "        x = basic_block(x, filters, stride=1, name=f'{name}_block{i+1}')\n",
        "\n",
        "    return x\n",
        "\n",
        "# Build the ResNet-15 model\n",
        "def build_resnet15(input_shape=(32, 32, 2), num_classes=2):\n",
        "    \"\"\"\n",
        "    Build a ResNet-15 model for image classification.\n",
        "\n",
        "    Args:\n",
        "        input_shape: Shape of the input images (height, width, channels)\n",
        "        num_classes: Number of output classes\n",
        "\n",
        "    Returns:\n",
        "        A Keras Model\n",
        "    \"\"\"\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolutional layer\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=1, padding='same',use_bias=False, name='conv1')(inputs)\n",
        "    x = layers.BatchNormalization(name='bn1')(x)\n",
        "    x = layers.ReLU(name='relu1')(x)\n",
        "\n",
        "    # Create ResNet layers\n",
        "    x = resnet_layer(x, blocks=3, filters=64, stride=1, name='layer1')\n",
        "    x = resnet_layer(x, blocks=4, filters=128, stride=2, name='layer2')\n",
        "    x = resnet_layer(x, blocks=6, filters=256, stride=2, name='layer3')\n",
        "\n",
        "    # Global Average Pooling and final fully connected layer\n",
        "    x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax', name='fc')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='ResNet15')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "f30e56dc-803a-4bd4-b40f-d437e12a3baa",
      "metadata": {
        "id": "f30e56dc-803a-4bd4-b40f-d437e12a3baa"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1:]  # (height, width, channels)\n",
        "model = build_resnet15(input_shape = input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2f0066ff-5800-4051-8208-18cf46f53734",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2f0066ff-5800-4051-8208-18cf46f53734",
        "outputId": "0ef143b0-acfd-441f-f70b-6bfc730a0123"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"ResNet15\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"ResNet15\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)              </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">        Param # </span><span style=\"font-weight: bold\"> Connected to           </span>\n",
              "\n",
              " input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                      \n",
              "\n",
              " conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">2,304</span>  input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " bn1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              "\n",
              " relu1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              \n",
              "\n",
              " layer1_block1_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span>  relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block1_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block1_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block1_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block1_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer1_block1_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span>  layer1_block1_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block1_shortcut_c  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block1_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block1_shortcut_bn  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block1_shortcu \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block1_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block1_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer1_block1_shortcu \n",
              "\n",
              " layer1_block1_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block1_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer1_block2_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span>  layer1_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block2_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block2_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block2_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block2_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer1_block2_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span>  layer1_block2_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block2_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block2_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block2_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer1_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer1_block2_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block2_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer1_block3_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span>  layer1_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block3_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block3_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block3_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block3_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer1_block3_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span>  layer1_block3_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer1_block3_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>  layer1_block3_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer1_block3_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block3_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer1_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer1_block3_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer1_block3_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block1_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span>  layer1_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block1_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block1_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block1_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block1_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block1_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block1_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block1_shortcut_c  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span>  layer1_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block1_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block1_shortcut_bn  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block1_shortcu \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block1_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block1_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer2_block1_shortcu \n",
              "\n",
              " layer2_block1_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block1_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block2_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block2_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block2_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block2_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block2_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block2_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block2_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block2_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block2_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block2_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer2_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer2_block2_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block2_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block3_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block3_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block3_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block3_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block3_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block3_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block3_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block3_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block3_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block3_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block3_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer2_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer2_block3_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block3_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block4_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block4_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block4_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block4_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block4_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer2_block4_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span>  layer2_block4_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer2_block4_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>  layer2_block4_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer2_block4_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block4_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer2_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer2_block4_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer2_block4_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block1_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">294,912</span>  layer2_block4_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block1_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block1_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block1_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block1_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block1_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block1_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block1_shortcut_c  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">32,768</span>  layer2_block4_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block1_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block1_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block1_shortcut_bn  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block1_shortcu \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block1_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block1_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block1_shortcu \n",
              "\n",
              " layer3_block1_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block1_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block2_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block2_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block2_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block2_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block2_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block2_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block2_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block2_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block2_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block2_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block2_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block1_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer3_block2_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block2_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block3_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block3_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block3_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block3_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block3_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block3_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block3_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block3_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block3_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block3_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block3_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block2_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer3_block3_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block3_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block4_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block4_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block4_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block4_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block4_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block4_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block4_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block4_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block4_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block4_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block4_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block3_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer3_block4_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block4_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block5_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block4_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block5_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block5_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block5_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block5_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block5_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block5_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block5_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block5_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block5_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block5_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block4_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer3_block5_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block5_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block6_conv1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block5_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block6_bn1          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block6_conv1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block6_relu1        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block6_bn1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " layer3_block6_conv2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span>  layer3_block6_relu1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                                                                                  \n",
              "\n",
              " layer3_block6_bn2          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span>  layer3_block6_conv2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                                                                      \n",
              "\n",
              " layer3_block6_add (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block6_bn2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              "                                                                    layer3_block5_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              "\n",
              " layer3_block6_relu2        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block6_add[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                                                                                    \n",
              "\n",
              " avg_pool                   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  layer3_block6_relu2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)                                                                  \n",
              "\n",
              " fc (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span>  avg_pool[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_layer (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m2\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  -                      \n",
              "\n",
              " conv1 (\u001b[38;5;33mConv2D\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m2,304\u001b[0m  input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " bn1 (\u001b[38;5;33mBatchNormalization\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  conv1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              "\n",
              " relu1 (\u001b[38;5;33mReLU\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              \n",
              "\n",
              " layer1_block1_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m73,728\u001b[0m  relu1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block1_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block1_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block1_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block1_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer1_block1_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m36,864\u001b[0m  layer1_block1_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block1_shortcut_c  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)               \u001b[38;5;34m8,192\u001b[0m  relu1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block1_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block1_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block1_shortcut_bn  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block1_shortcu \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block1_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block1_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer1_block1_shortcu \n",
              "\n",
              " layer1_block1_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block1_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer1_block2_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m36,864\u001b[0m  layer1_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block2_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block2_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block2_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block2_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer1_block2_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m36,864\u001b[0m  layer1_block2_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block2_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block2_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block2_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block2_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer1_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer1_block2_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block2_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer1_block3_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m36,864\u001b[0m  layer1_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block3_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block3_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block3_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block3_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer1_block3_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m36,864\u001b[0m  layer1_block3_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer1_block3_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                 \u001b[38;5;34m256\u001b[0m  layer1_block3_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer1_block3_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block3_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer1_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer1_block3_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  layer1_block3_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block1_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             \u001b[38;5;34m73,728\u001b[0m  layer1_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block1_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block1_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block1_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block1_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block1_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block1_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block1_shortcut_c  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)              \u001b[38;5;34m8,192\u001b[0m  layer1_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block1_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block1_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block1_shortcut_bn  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block1_shortcu \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block1_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block1_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer2_block1_shortcu \n",
              "\n",
              " layer2_block1_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block1_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block2_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block2_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block2_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block2_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block2_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block2_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block2_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block2_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block2_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block2_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block2_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer2_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer2_block2_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block2_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block3_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block3_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block3_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block3_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block3_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block3_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block3_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block3_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block3_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block3_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block3_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer2_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer2_block3_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block3_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block4_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block4_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block4_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block4_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block4_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer2_block4_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m147,456\u001b[0m  layer2_block4_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer2_block4_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                \u001b[38;5;34m512\u001b[0m  layer2_block4_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer2_block4_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block4_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer2_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer2_block4_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  layer2_block4_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block1_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m294,912\u001b[0m  layer2_block4_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block1_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block1_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block1_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block1_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block1_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block1_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block1_shortcut_c  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)               \u001b[38;5;34m32,768\u001b[0m  layer2_block4_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block1_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block1_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block1_shortcut_bn  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block1_shortcu \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block1_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block1_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block1_shortcu \n",
              "\n",
              " layer3_block1_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block1_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block2_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block2_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block2_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block2_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block2_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block2_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block2_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block2_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block2_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block2_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block2_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block1_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer3_block2_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block2_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block3_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block3_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block3_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block3_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block3_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block3_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block3_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block3_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block3_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block3_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block3_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block2_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer3_block3_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block3_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block4_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block4_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block4_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block4_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block4_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block4_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block4_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block4_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block4_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block4_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block4_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block3_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer3_block4_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block4_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block5_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block4_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block5_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block5_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block5_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block5_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block5_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block5_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block5_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block5_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block5_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block5_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block4_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer3_block5_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block5_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block6_conv1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block5_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block6_bn1          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block6_conv1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block6_relu1        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block6_bn1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " layer3_block6_conv2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)              \u001b[38;5;34m589,824\u001b[0m  layer3_block6_relu1[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mConv2D\u001b[0m)                                                                                  \n",
              "\n",
              " layer3_block6_bn2          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                \u001b[38;5;34m1,024\u001b[0m  layer3_block6_conv2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mBatchNormalization\u001b[0m)                                                                      \n",
              "\n",
              " layer3_block6_add (\u001b[38;5;33mAdd\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block6_bn2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              "                                                                    layer3_block5_relu2[\u001b[38;5;34m0\u001b[0m \n",
              "\n",
              " layer3_block6_relu2        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m0\u001b[0m  layer3_block6_add[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mReLU\u001b[0m)                                                                                    \n",
              "\n",
              " avg_pool                   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                          \u001b[38;5;34m0\u001b[0m  layer3_block6_relu2[\u001b[38;5;34m0\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)                                                                  \n",
              "\n",
              " fc (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                          \u001b[38;5;34m514\u001b[0m  avg_pool[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,219,138</span> (31.35 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,219,138\u001b[0m (31.35 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,209,026</span> (31.31 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,209,026\u001b[0m (31.31 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,112</span> (39.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m10,112\u001b[0m (39.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bb2a45d7-67b6-4920-a216-4f6b9c3ff5bc",
      "metadata": {
        "id": "bb2a45d7-67b6-4920-a216-4f6b9c3ff5bc"
      },
      "outputs": [],
      "source": [
        "# Custom OneCycleLR learning rate scheduler\n",
        "def one_cycle_lr_scheduler(max_lr, epochs, steps_per_epoch, pct_start=0.3, div_factor=10, final_div_factor=100):\n",
        "    \"\"\"\n",
        "    Creates a callback for the One Cycle LR policy.\n",
        "\n",
        "    Args:\n",
        "        max_lr: Maximum learning rate\n",
        "        epochs: Total number of epochs\n",
        "        steps_per_epoch: Number of steps per epoch\n",
        "        pct_start: Percentage of iterations for learning rate warmup\n",
        "        div_factor: Initial learning rate is max_lr/div_factor\n",
        "        final_div_factor: Final learning rate is max_lr/(div_factor*final_div_factor)\n",
        "\n",
        "    Returns:\n",
        "        A callback that adjusts the learning rate\n",
        "    \"\"\"\n",
        "    total_steps = epochs * steps_per_epoch\n",
        "    warmup_steps = int(total_steps * pct_start)\n",
        "    cooldown_steps = total_steps - warmup_steps\n",
        "\n",
        "    initial_lr = max_lr / div_factor\n",
        "    final_lr = max_lr / (div_factor * final_div_factor)\n",
        "\n",
        "    def lr_schedule(epoch, lr):\n",
        "        # Convert epoch to step\n",
        "        step = epoch * steps_per_epoch\n",
        "\n",
        "        if step < warmup_steps:\n",
        "            # Linear increase from initial_lr to max_lr\n",
        "            return initial_lr + (max_lr - initial_lr) * step / warmup_steps\n",
        "        else:\n",
        "            # Cosine annealing from max_lr to final_lr\n",
        "            x = (step - warmup_steps) / cooldown_steps\n",
        "            return final_lr + 0.5 * (max_lr - final_lr) * (1 + np.cos(np.pi * x))\n",
        "\n",
        "    return LearningRateScheduler(lr_schedule, verbose=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17c6d4f1-2597-40f1-8266-58d9c25dd5ed",
      "metadata": {
        "id": "17c6d4f1-2597-40f1-8266-58d9c25dd5ed"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, X_train, y_train, X_test, y_test, batch_size=32, epochs=30):\n",
        "    \"\"\"\n",
        "    Train the model with the provided data.\n",
        "\n",
        "    Args:\n",
        "        model: The Keras model to train\n",
        "        X_train, y_train: Training data and labels\n",
        "        X_test, y_test: Test data and labels\n",
        "        batch_size: Batch size for training\n",
        "        epochs: Number of epochs to train for\n",
        "\n",
        "    Returns:\n",
        "        Training history and the trained model\n",
        "    \"\"\"\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=0.0001),\n",
        "        # loss='categorical_crossentropy',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # Calculate steps per epoch\n",
        "    steps_per_epoch = len(X_train) // batch_size\n",
        "\n",
        "    # Create callbacks\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(\n",
        "            'best_model.h5',\n",
        "            monitor='val_accuracy',\n",
        "            verbose=1,\n",
        "            save_best_only=True,\n",
        "            mode='max'\n",
        "        ),\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-6\n",
        "        ),\n",
        "        one_cycle_lr_scheduler(\n",
        "            max_lr=0.01,\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=steps_per_epoch\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train, #y_train_onehot\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(X_test, y_test), #y_test_onehot\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Load the best model\n",
        "    model = tf.keras.models.load_model('best_model.h5')\n",
        "\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "349e5b78-d83a-4d44-9a62-f73effc357a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "collapsed": true,
        "id": "349e5b78-d83a-4d44-9a62-f73effc357a6",
        "outputId": "fe1168c0-9d8f-484d-d0fe-14a7e6855097"
      },
      "outputs": [],
      "source": [
        "history, model = train_model(model, X_train, y_train, X_test, y_test, epochs=30)\n",
        "\n",
        "# train_model(model, X_train, y_train, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f676fbf1-a3e2-4b6d-917b-ea3abbdfb8b3",
      "metadata": {
        "id": "f676fbf1-a3e2-4b6d-917b-ea3abbdfb8b3"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "import seaborn as sns\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss and accuracy.\n",
        "\n",
        "    Args:\n",
        "        history: History object returned by model.fit()\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Loss plot\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Loss Curves')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Accuracy plot\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Accuracy Curves')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vmFvv8COsPY6",
      "metadata": {
        "id": "vmFvv8COsPY6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the model and display various metrics and visualizations.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        X_test, y_test: Test data and labels\n",
        "\n",
        "    Returns:\n",
        "        Dictionary containing evaluation metrics\n",
        "    \"\"\"\n",
        "    # Convert labels to one-hot encoded format for evaluation\n",
        "    y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=2)\n",
        "\n",
        "    # Get model predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Calculate test accuracy\n",
        "    test_loss, test_acc = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
        "    print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Electron', 'Photon']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Electron', 'Photon'], yticklabels=['Electron', 'Photon'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Normalized confusion matrix\n",
        "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=['Electron', 'Photon'], yticklabels=['Electron', 'Photon'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Normalized Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve and AUC\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_probs[:, 1])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'test_accuracy': test_acc,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "h9SC4kobsPbw",
      "metadata": {
        "id": "h9SC4kobsPbw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualize sample images\n",
        "def visualize_sample_images(X, y, indices=None, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize sample images from the dataset.\n",
        "\n",
        "    Args:\n",
        "        X: Image data (samples, height, width, channels)\n",
        "        y: Labels\n",
        "        indices: Specific indices to visualize (optional)\n",
        "        num_samples: Number of samples to visualize if indices not provided\n",
        "    \"\"\"\n",
        "    if indices is None:\n",
        "        indices = np.random.choice(len(X), num_samples, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    for i, idx in enumerate(indices):\n",
        "        img = X[idx]\n",
        "        label = y[idx]\n",
        "\n",
        "        # Print image statistics\n",
        "        print(f\"Sample {idx} shape: {img.shape}\")\n",
        "        print(f\"Channel 0 (Energy) - Min: {img[..., 0].min():.4f}, Max: {img[..., 0].max():.4f}, Mean: {img[..., 0].mean():.4f}\")\n",
        "        print(f\"Channel 1 (Time) - Min: {img[..., 1].min():.4f}, Max: {img[..., 1].max():.4f}, Mean: {img[..., 1].mean():.4f}\")\n",
        "\n",
        "        # Plot energy channel\n",
        "        plt.subplot(num_samples, 3, i*3 + 1)\n",
        "        plt.title(f\"Class: {'Electron' if label == 0 else 'Photon'}\")\n",
        "        plt.imshow(img[..., 0], cmap='viridis')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "        plt.title('Energy Channel')\n",
        "\n",
        "        # Plot time channel\n",
        "        plt.subplot(num_samples, 3, i*3 + 2)\n",
        "        plt.imshow(img[..., 1], cmap='plasma')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "        plt.title('Time Channel')\n",
        "\n",
        "        # Combined channels (RGB visualization)\n",
        "        plt.subplot(num_samples, 3, i*3 + 3)\n",
        "        # Create a 3-channel RGB image for visualization\n",
        "        h, w = img.shape[:2]\n",
        "        rgb_img = np.zeros((h, w, 3))\n",
        "        rgb_img[..., 0] = img[..., 0]  # Energy in R channel\n",
        "        rgb_img[..., 1] = img[..., 1]  # Time in G channel\n",
        "        plt.imshow(rgb_img)\n",
        "        plt.axis('off')\n",
        "        plt.title('Combined Channels')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lY68Ay-HsPed",
      "metadata": {
        "id": "lY68Ay-HsPed"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualize misclassified samples\n",
        "def misclassified_samples(model, X_test, y_test, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize misclassified samples.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        X_test, y_test: Test data and labels\n",
        "        num_samples: Number of misclassified samples to visualize\n",
        "    \"\"\"\n",
        "    # Get predictions\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # Find misclassified samples\n",
        "    misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "    if len(misclassified_indices) == 0:\n",
        "        print(\"No misclassified samples found!\")\n",
        "        return\n",
        "\n",
        "    # Limit to requested number of samples\n",
        "    indices = misclassified_indices[:num_samples]\n",
        "\n",
        "    # Visualize the misclassified samples\n",
        "    visualize_sample_images(X_test, y_test, indices, len(indices))\n",
        "\n",
        "    # Show prediction details\n",
        "    for i, idx in enumerate(indices):\n",
        "        print(f\"Sample {idx}:\")\n",
        "        print(f\"  True label: {'Electron' if y_test[idx] == 0 else 'Photon'}\")\n",
        "        print(f\"  Predicted: {'Electron' if y_pred[idx] == 0 else 'Photon'}\")\n",
        "        print(f\"  Probabilities: Electron: {y_pred_probs[idx][0]:.4f}, Photon: {y_pred_probs[idx][1]:.4f}\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5HQapUgCsPgn",
      "metadata": {
        "id": "5HQapUgCsPgn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Feature importance visualization with Grad-CAM\n",
        "def visualize_feature_importance(model, X_test, y_test, num_samples=5):\n",
        "    \"\"\"\n",
        "    Visualize feature importance using Grad-CAM.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Keras model\n",
        "        X_test, y_test: Test data and labels\n",
        "        num_samples: Number of samples to visualize\n",
        "    \"\"\"\n",
        "    # Create a model that outputs both the predictions and the last conv layer activations\n",
        "    last_conv_layer = None\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, layers.Conv2D):\n",
        "            last_conv_layer = layer.name\n",
        "            break\n",
        "\n",
        "    if last_conv_layer is None:\n",
        "        print(\"Could not find a convolutional layer\")\n",
        "        return\n",
        "\n",
        "    # Create a Grad-CAM model\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        inputs=[model.inputs],\n",
        "        outputs=[model.get_layer(last_conv_layer).output, model.output]\n",
        "    )\n",
        "\n",
        "    # Select random samples\n",
        "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
        "\n",
        "    for idx in indices:\n",
        "        img = X_test[idx]\n",
        "        label = y_test[idx]\n",
        "        img_array = np.expand_dims(img, axis=0)\n",
        "\n",
        "        # Get predictions and feature map\n",
        "        with tf.GradientTape() as tape:\n",
        "            conv_outputs, predictions = grad_model(img_array)\n",
        "            class_idx = np.argmax(predictions[0])\n",
        "            loss = predictions[:, class_idx]\n",
        "\n",
        "        # Extract gradients\n",
        "        grads = tape.gradient(loss, conv_outputs)\n",
        "\n",
        "        # Global average pooling of the gradients\n",
        "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "        # Weight feature maps with the gradients\n",
        "        conv_outputs = conv_outputs[0]\n",
        "        heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
        "        heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "        heatmap = heatmap.numpy()\n",
        "\n",
        "        # Resize heatmap to match input image size\n",
        "        heatmap = np.uint8(255 * heatmap)\n",
        "        heatmap = tf.image.resize(\n",
        "            np.expand_dims(heatmap, axis=-1),\n",
        "            (img.shape[0], img.shape[1])\n",
        "        ).numpy().squeeze()\n",
        "\n",
        "        # Visualize\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Original image - energy channel\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(f\"Original - {'Electron' if label == 0 else 'Photon'}\")\n",
        "        plt.imshow(img[..., 0], cmap='viridis')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Original image - time channel\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(\"Time Channel\")\n",
        "        plt.imshow(img[..., 1], cmap='plasma')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "\n",
        "        # Heatmap\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(\"Feature Importance\")\n",
        "        plt.imshow(heatmap, cmap='hot')\n",
        "        plt.colorbar(fraction=0.046, pad=0.04)\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Sample {idx}:\")\n",
        "        print(f\"  True label: {'Electron' if label == 0 else 'Photon'}\")\n",
        "        pred_class = 'Electron' if class_idx == 0 else 'Photon'\n",
        "        print(f\"  Predicted: {pred_class} with confidence {predictions[0][class_idx]:.4f}\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c82dfa88-d27b-4ad3-a0aa-9451c1573ed4",
      "metadata": {
        "id": "c82dfa88-d27b-4ad3-a0aa-9451c1573ed4"
      },
      "outputs": [],
      "source": [
        "plot_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bbc86e-8a12-4a11-ae95-cb1d02b8a64d",
      "metadata": {
        "id": "16bbc86e-8a12-4a11-ae95-cb1d02b8a64d"
      },
      "outputs": [],
      "source": [
        "metrics = evaluate_model(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d513c47f-e150-4e73-9c90-dcff142b9788",
      "metadata": {
        "id": "d513c47f-e150-4e73-9c90-dcff142b9788"
      },
      "outputs": [],
      "source": [
        "misclassified_samples(model, X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a74aeb7e-faf3-4c19-8471-6c79a66adb09",
      "metadata": {
        "id": "a74aeb7e-faf3-4c19-8471-6c79a66adb09"
      },
      "outputs": [],
      "source": [
        "visualize_feature_importance(model, X_test, y_test)\n",
        "print(\"Analysis complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4QyMfOqfsniy",
      "metadata": {
        "id": "4QyMfOqfsniy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vNajurCgsnoa",
      "metadata": {
        "id": "vNajurCgsnoa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0jNfd2_FsnvJ",
      "metadata": {
        "id": "0jNfd2_FsnvJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace2cdd1-402e-4a89-8620-759771105f8a",
      "metadata": {
        "id": "ace2cdd1-402e-4a89-8620-759771105f8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c90a8f9e-d009-4d70-8ce4-996df4d41119",
      "metadata": {
        "id": "c90a8f9e-d009-4d70-8ce4-996df4d41119"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4448229c-b1ae-413b-9682-0e706215cc3f",
      "metadata": {
        "id": "4448229c-b1ae-413b-9682-0e706215cc3f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CMsYBYnabYGi",
      "metadata": {
        "id": "CMsYBYnabYGi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rGoIjoRxbYJY",
      "metadata": {
        "id": "rGoIjoRxbYJY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ObVMGvRbYMC",
      "metadata": {
        "id": "9ObVMGvRbYMC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3svemmf2bYOs",
      "metadata": {
        "id": "3svemmf2bYOs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ThbPqM_bYRh",
      "metadata": {
        "id": "6ThbPqM_bYRh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xmKWRvuJbYUM",
      "metadata": {
        "id": "xmKWRvuJbYUM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d94bI-L5bYW9",
      "metadata": {
        "id": "d94bI-L5bYW9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eokgNvYKbYZl",
      "metadata": {
        "id": "eokgNvYKbYZl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WGLo5NlPbYcf",
      "metadata": {
        "id": "WGLo5NlPbYcf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_6zKEcAHbYfF",
      "metadata": {
        "id": "_6zKEcAHbYfF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZSt-YzRmbYh1",
      "metadata": {
        "id": "ZSt-YzRmbYh1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NBONfQd-bYk1",
      "metadata": {
        "id": "NBONfQd-bYk1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2W_t-IObYnV",
      "metadata": {
        "id": "m2W_t-IObYnV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iSDUdw6KbYp7",
      "metadata": {
        "id": "iSDUdw6KbYp7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "G8r5n81ObYso",
      "metadata": {
        "id": "G8r5n81ObYso"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cWVPUAYIbYvK",
      "metadata": {
        "id": "cWVPUAYIbYvK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v97cLmn8bYxy",
      "metadata": {
        "id": "v97cLmn8bYxy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Rh2MVH2PbY0s",
      "metadata": {
        "id": "Rh2MVH2PbY0s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMyqq-8NbY3T",
      "metadata": {
        "id": "zMyqq-8NbY3T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f829b1fb-0309-4f29-ad5a-d5977e51ac7f",
      "metadata": {
        "id": "f829b1fb-0309-4f29-ad5a-d5977e51ac7f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39c90bd2-992f-451a-bd5a-4ea99aa418b2",
      "metadata": {
        "id": "39c90bd2-992f-451a-bd5a-4ea99aa418b2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2683068-d6bb-4562-bf61-b8127b5b8184",
      "metadata": {
        "id": "e2683068-d6bb-4562-bf61-b8127b5b8184"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c73e99a-53eb-4fe5-8b2e-622a4cdf6331",
      "metadata": {
        "id": "8c73e99a-53eb-4fe5-8b2e-622a4cdf6331"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c8eb11-f12a-42da-99b9-7d62122d2a64",
      "metadata": {
        "id": "60c8eb11-f12a-42da-99b9-7d62122d2a64"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "335b96bd-3e88-4806-98e4-6064a61c8080",
      "metadata": {
        "id": "335b96bd-3e88-4806-98e4-6064a61c8080"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6c8d002-ff88-4cc7-8d52-6375e73451ed",
      "metadata": {
        "id": "b6c8d002-ff88-4cc7-8d52-6375e73451ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1ed5cd-8d76-44ca-91d7-fc2ffaa0f828",
      "metadata": {
        "id": "bb1ed5cd-8d76-44ca-91d7-fc2ffaa0f828"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e518d3e-bc23-48dd-83e2-76bb47eaa26b",
      "metadata": {
        "id": "1e518d3e-bc23-48dd-83e2-76bb47eaa26b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
